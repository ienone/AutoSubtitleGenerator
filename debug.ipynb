{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c30ae2c-a47e-4f92-ba9e-150e002bc12a",
   "metadata": {},
   "source": [
    "## 一 配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a66c211-2f35-433d-bafd-6782876f2107",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-15 01:24:41 - INFO - --- 初始化配置 ---\n",
      "2025-07-15 01:24:41 - INFO - 正在创建所需的工作目录...\n",
      "2025-07-15 01:24:41 - INFO -  > 工作目录已确认: /root/autodl-tmp/Sub_Gen/workspace\n",
      "2025-07-15 01:24:41 - INFO - 正在从 'GrandBlue' 目录中搜索视频文件...\n",
      "2025-07-15 01:24:41 - INFO - 成功发现 12 个视频文件待处理:\n",
      "2025-07-15 01:24:41 - INFO -   - S02E01.mp4\n",
      "2025-07-15 01:24:41 - INFO -   - S02E02.mp4\n",
      "2025-07-15 01:24:41 - INFO -   - S02E03.mp4\n",
      "2025-07-15 01:24:41 - INFO -   - S02E04.mp4\n",
      "2025-07-15 01:24:41 - INFO -   - S02E05.mp4\n",
      "2025-07-15 01:24:41 - INFO -   - S02E06.mp4\n",
      "2025-07-15 01:24:41 - INFO -   - S02E07.mp4\n",
      "2025-07-15 01:24:41 - INFO -   - S02E08.mp4\n",
      "2025-07-15 01:24:41 - INFO -   - S02E09.mp4\n",
      "2025-07-15 01:24:41 - INFO -   - S02E10.mp4\n",
      "2025-07-15 01:24:41 - INFO -   - S02E11.mp4\n",
      "2025-07-15 01:24:41 - INFO -   - S02E12.mp4\n",
      "2025-07-15 01:24:41 - INFO -  > PyTorch 将使用设备: CUDA\n",
      "2025-07-15 01:24:41 - INFO -  > GPU 名称: NVIDIA GeForce RTX 3090\n",
      "2025-07-15 01:24:41 - INFO -  > DeepSeek API 密钥已配置。\n",
      "2025-07-15 01:24:41 - INFO - --- 配置完成，环境已就绪 ---\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# 1. 环境检查与库安装 (在终端已完成，此处仅作记录)\n",
    "# ===================================================================\n",
    "# 以下命令已在 Conda 虚拟环境 anisub 中执行完毕，无需在此重新运行。\n",
    "# !conda install python=3.10 ffmpeg pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia -c conda-forge\n",
    "# !conda install -c conda-forge demucs requests pydub ipywidgets jupyter ipykernel\n",
    "# !pip install openai-whisper\n",
    "# !pip install git+https://github.com/snakers4/silero-vad.git\n",
    "\n",
    "# ===================================================================\n",
    "# 2. 导入所有必需的库\n",
    "# ===================================================================\n",
    "import os\n",
    "import torch\n",
    "import whisper\n",
    "import requests\n",
    "import json\n",
    "import subprocess\n",
    "import demucs\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from pydub import AudioSegment\n",
    "from IPython.display import Audio, display\n",
    "import sys \n",
    "\n",
    "# ===================================================================\n",
    "# 3. 日志配置\n",
    "# ===================================================================\n",
    "# 配置日志记录器，方便在长时任务中追踪进度和排查问题\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "# ===================================================================\n",
    "# 4. 全局变量与路径配置\n",
    "# ===================================================================\n",
    "logging.info(\"--- 初始化配置 ---\")\n",
    "\n",
    "# --- 核心目录配置 ---\n",
    "# 输入目录：存放所有待处理的视频文件\n",
    "INPUT_DIR = Path(\"GrandBlue\")\n",
    "# 工作目录：所有中间文件和最终产物都将保存在这里\n",
    "WORKSPACE_DIR = Path(\"workspace\")\n",
    "TEMP_DIR = WORKSPACE_DIR / \"temp\"\n",
    "OUTPUT_DIR = WORKSPACE_DIR / \"output\"\n",
    "\n",
    "# 注意：由于现在是批量处理，具体到每个视频的文件路径（如 full_audio.wav, vocals.wav 等）\n",
    "# 将在后续的处理循环中根据当前视频动态生成，因此不在全局预先定义。\n",
    "\n",
    "# --- API 密钥 ---\n",
    "# !!! 重要: 请在此处填入您的 DeepSeek API 密钥 !!!\n",
    "DEEPSEEK_API_KEY = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
    "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"hf-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
    "# ===================================================================\n",
    "# 5. 初始化环境与发现输入文件\n",
    "# ===================================================================\n",
    "# --- 创建工作目录 ---\n",
    "logging.info(\"正在创建所需的工作目录...\")\n",
    "INPUT_DIR.mkdir(exist_ok=True) # 确保输入目录也存在\n",
    "WORKSPACE_DIR.mkdir(exist_ok=True)\n",
    "TEMP_DIR.mkdir(exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "logging.info(f\" > 工作目录已确认: {WORKSPACE_DIR.resolve()}\")\n",
    "\n",
    "# --- 发现所有视频文件 ---\n",
    "logging.info(f\"正在从 '{INPUT_DIR}' 目录中搜索视频文件...\")\n",
    "supported_extensions = ['.mp4', '.mkv', '.avi', '.mov', '.webm', '.flv']\n",
    "video_files = []\n",
    "for ext in supported_extensions:\n",
    "    video_files.extend(INPUT_DIR.glob(f\"*{ext}\"))\n",
    "\n",
    "if not video_files:\n",
    "    logging.warning(f\"在 '{INPUT_DIR}' 目录中未找到任何支持的视频文件。\")\n",
    "    logging.warning(\"请将您的 .mp4, .mkv 等文件放入该目录。\")\n",
    "else:\n",
    "    logging.info(f\"成功发现 {len(video_files)} 个视频文件待处理:\")\n",
    "    for video_path in video_files:\n",
    "        logging.info(f\"  - {video_path.name}\")\n",
    "\n",
    "# --- 检查并设置计算设备 (CPU/GPU) ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "logging.info(f\" > PyTorch 将使用设备: {DEVICE.upper()}\")\n",
    "if DEVICE == \"cuda\":\n",
    "    logging.info(f\" > GPU 名称: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# --- 检查API密钥 ---\n",
    "if \"sk-xxxxxxxx\" in DEEPSEEK_API_KEY:\n",
    "    logging.warning(\" > 注意: DeepSeek API 密钥似乎是占位符，请记得在后续步骤前填充。\")\n",
    "else:\n",
    "    logging.info(\" > DeepSeek API 密钥已配置。\")\n",
    "    \n",
    "logging.info(\"--- 配置完成，环境已就绪 ---\")\n",
    "\n",
    "# ===================================================================\n",
    "# 流程变更重要提示\n",
    "# ===================================================================\n",
    "# 从下一个单元格开始，您的所有代码（步骤1到步骤9）\n",
    "# 都应该被包含在一个 for 循环中，像这样：\n",
    "#\n",
    "# for video_path in video_files:\n",
    "#     logging.info(f\"===== 开始处理视频: {video_path.name} =====\")\n",
    "#     \n",
    "#     # --- 在这里根据 video_path 动态定义该视频的文件路径 ---\n",
    "#     video_stem = video_path.stem\n",
    "#     full_audio_path = TEMP_DIR / f\"{video_stem}_full_audio.wav\"\n",
    "#     vocals_path = TEMP_DIR / \"htdemucs\" / video_stem / \"vocals.wav\"\n",
    "#     # ... 其他路径 ...\n",
    "#\n",
    "#     # --- 然后执行您的步骤 1: 提取音频 ---\n",
    "#     # extract_audio(video_path, full_audio_path)\n",
    "#     \n",
    "#     # --- 接着执行步骤 2, 3, 4... ---\n",
    "#     # ...\n",
    "#\n",
    "#     logging.info(f\"===== 视频 {video_path.name} 处理完成 =====\")\n",
    "#\n",
    "# ==================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb966d07-7b7d-4513-96e9-d448377abf33",
   "metadata": {},
   "source": [
    "## 二 音频预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31e9b4b2-69da-4081-beb3-32735c4a8312",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-15 01:24:41 - INFO - --- [测试开始] 运行优化后的步骤1, 2, 和 2.5 ---\n",
      "2025-07-15 01:24:41 - INFO - 将使用第一个视频进行测试: 'S02E01.mp4'\n",
      "2025-07-15 01:24:41 - INFO - 高质量音频文件已存在，跳过提取: S02E01.wav\n",
      "2025-07-15 01:24:41 - INFO - 高质量人声文件已存在，跳过分离: vocals.wav\n",
      "2025-07-15 01:24:41 - INFO - 优化后的人声文件已存在，跳过转换: vocals_16k_mono.wav\n",
      "2025-07-15 01:24:41 - INFO - --- [测试成功] ---\n",
      "2025-07-15 01:24:41 - INFO - 所有阶段（提取、分离、优化）均按预期工作。\n",
      "2025-07-15 01:24:41 - INFO - 后续的 VAD 和 Whisper 步骤将使用这个优化文件: vocals_16k_mono.wav\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# 步骤 1, 2, 和 2.5 的功能函数 (已优化)\n",
    "# ===================================================================\n",
    "\n",
    "def extract_audio(video_path: Path, audio_path: Path):\n",
    "    \"\"\"步骤1：从视频提取高质量音轨 (44.1kHz, 立体声) 以便Demucs处理。\"\"\"\n",
    "    if audio_path.exists():\n",
    "        logging.info(f\"高质量音频文件已存在，跳过提取: {audio_path.name}\")\n",
    "        return True\n",
    "    logging.info(f\"为Demucs提取高质量音频: '{video_path.name}' -> '{audio_path.name}'\")\n",
    "    command = [\n",
    "        \"ffmpeg\", \"-i\", str(video_path), \n",
    "        \"-vn\", \"-acodec\", \"pcm_s16le\", \"-ar\", \"44100\", \"-ac\", \"2\", \n",
    "        str(audio_path), \"-y\"\n",
    "    ]\n",
    "    try:\n",
    "        subprocess.run(command, check=True, capture_output=True, text=True)\n",
    "        logging.info(f\"高质量音轨提取成功。大小: {audio_path.stat().st_size / 1e6:.2f} MB\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        logging.error(f\"提取 '{video_path.name}' 的音频时出错:\\n{e.stderr}\")\n",
    "        return False\n",
    "\n",
    "def separate_vocals(full_audio_path: Path, output_dir: Path, device: str):\n",
    "    \"\"\"步骤2：使用Demucs分离人声，输入为高质量音频。\"\"\"\n",
    "    audio_stem = full_audio_path.stem.replace('.wav', '') # 确保stem干净\n",
    "    vocals_path = output_dir / \"htdemucs\" / audio_stem / \"vocals.wav\"\n",
    "    if vocals_path.exists():\n",
    "        logging.info(f\"高质量人声文件已存在，跳过分离: {vocals_path.name}\")\n",
    "        return vocals_path\n",
    "    logging.info(f\"开始使用 Demucs 分离人声: {full_audio_path.name}\")\n",
    "    demucs_command = [\n",
    "        sys.executable, \"-m\", \"demucs.separate\",\n",
    "        \"-n\", \"htdemucs\", \"--two-stems\", \"vocals\",\n",
    "        \"-d\", device, \"--out\", str(output_dir), str(full_audio_path)\n",
    "    ]\n",
    "    try:\n",
    "        subprocess.run(demucs_command, check=True)\n",
    "        logging.info(f\"人声分离完成。高质量人声文件: {vocals_path}\")\n",
    "        return vocals_path\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        logging.error(f\"Demucs 处理 '{full_audio_path.name}' 时失败。\")\n",
    "        return None\n",
    "\n",
    "def optimize_audio_for_transcription(raw_vocals_path: Path, temp_dir: Path):\n",
    "    \"\"\"\n",
    "    步骤2.5 (新增): 将高质量人声WAV转换为轻量级版本 (16kHz, 单声道)。\n",
    "    这会极大减小文件大小，且是VAD和Whisper的最佳格式。\n",
    "    \"\"\"\n",
    "    # e.g., \"S02E01.wav\" -> \"S02E01_16k_mono.wav\"\n",
    "    optimized_path = temp_dir / f\"{raw_vocals_path.stem}_16k_mono.wav\"\n",
    "    \n",
    "    if optimized_path.exists():\n",
    "        logging.info(f\"优化后的人声文件已存在，跳过转换: {optimized_path.name}\")\n",
    "        return optimized_path\n",
    "\n",
    "    logging.info(f\"开始优化人声文件用于转录: {raw_vocals_path.name}\")\n",
    "    logging.info(f\"原始大小: {raw_vocals_path.stat().st_size / 1e6:.2f} MB\")\n",
    "    \n",
    "    command = [\n",
    "        \"ffmpeg\", \"-i\", str(raw_vocals_path),\n",
    "        \"-ar\", \"16000\",          # 采样率降至 16kHz\n",
    "        \"-ac\", \"1\",              # 通道转为单声道\n",
    "        \"-c:a\", \"pcm_s16le\",     # 保持为WAV编码\n",
    "        str(optimized_path),\n",
    "        \"-y\"\n",
    "    ]\n",
    "    try:\n",
    "        subprocess.run(command, check=True, capture_output=True, text=True)\n",
    "        logging.info(f\"优化完成！新文件: {optimized_path.name}\")\n",
    "        logging.info(f\"优化后大小: {optimized_path.stat().st_size / 1e6:.2f} MB\")\n",
    "        return optimized_path\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        logging.error(f\"优化人声文件时出错:\\n{e.stderr}\")\n",
    "        return None\n",
    "\n",
    "# ===================================================================\n",
    "# 测试区 (已更新流程)\n",
    "# ===================================================================\n",
    "logging.info(\"--- [测试开始] 运行优化后的步骤1, 2, 和 2.5 ---\")\n",
    "\n",
    "if not video_files:\n",
    "    logging.warning(\"未找到任何视频文件，无法执行测试。\")\n",
    "else:\n",
    "    test_video_path = video_files[0]\n",
    "    # 从 \"S02E01.mp4\" 得到 \"S02E01\"\n",
    "    test_video_stem = test_video_path.stem\n",
    "    logging.info(f\"将使用第一个视频进行测试: '{test_video_path.name}'\")\n",
    "    \n",
    "    # 定义测试文件路径\n",
    "    # 完整的高质量音频，用于Demucs输入\n",
    "    test_full_audio_path = TEMP_DIR / f\"{test_video_stem}.wav\"\n",
    "\n",
    "    # --- 步骤 1 ---\n",
    "    if extract_audio(test_video_path, test_full_audio_path):\n",
    "        # --- 步骤 2 ---\n",
    "        raw_vocals_path = separate_vocals(test_full_audio_path, TEMP_DIR, DEVICE)\n",
    "        \n",
    "        if raw_vocals_path:\n",
    "            # --- 新增步骤 2.5: 优化音频 ---\n",
    "            optimized_vocals_path = optimize_audio_for_transcription(raw_vocals_path, TEMP_DIR)\n",
    "\n",
    "            if optimized_vocals_path:\n",
    "                logging.info(\"--- [测试成功] ---\")\n",
    "                logging.info(\"所有阶段（提取、分离、优化）均按预期工作。\")\n",
    "                logging.info(f\"后续的 VAD 和 Whisper 步骤将使用这个优化文件: {optimized_vocals_path.name}\")\n",
    "                # logging.info(\"下方是优化后的人声预览:\")\n",
    "                # display(Audio(optimized_vocals_path, autoplay=False))\n",
    "            else:\n",
    "                 logging.error(\"--- [测试失败] 在音频优化步骤 ---\")\n",
    "        else:\n",
    "            logging.error(\"--- [测试失败] 在人声分离步骤 ---\")\n",
    "    else:\n",
    "        logging.error(\"--- [测试失败] 在音频提取步骤 ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13b8d76-e70a-4625-81ab-4dd7da13ee83",
   "metadata": {},
   "source": [
    "## 三 VAD 静音检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71bf87f8-d3ad-48e3-8ada-1674f9733e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-15 01:24:42 - INFO - --- [测试开始] 步骤3：语音活动检测 (VAD) 与音频切割 ---\n",
      "2025-07-15 01:24:42 - INFO - 将对文件进行 VAD 处理: vocals_16k_mono.wav\n",
      "2025-07-15 01:24:42 - INFO - 正在加载 Silero VAD 模型...\n",
      "Using cache found in /root/.cache/torch/hub/snakers4_silero-vad_master\n",
      "2025-07-15 01:24:43 - INFO - 正在读取音频文件用于 VAD: vocals_16k_mono.wav\n",
      "2025-07-15 01:24:43 - INFO - 开始使用 Silero VAD (优化参数) 检测语音时间戳...\n",
      "2025-07-15 01:24:59 - INFO - VAD 检测到 393 个语音片段。\n",
      "2025-07-15 01:25:00 - INFO - --- [VAD成功] VAD 处理完成 ---\n",
      "2025-07-15 01:25:00 - INFO - 前5个检测到的语音片段 (单位: 毫秒):\n",
      "2025-07-15 01:25:00 - INFO -   - Start:     868 ms, End:    1724 ms\n",
      "2025-07-15 01:25:00 - INFO -   - Start:   47236 ms, End:   49724 ms\n",
      "2025-07-15 01:25:00 - INFO -   - Start:   51844 ms, End:   52636 ms\n",
      "2025-07-15 01:25:00 - INFO -   - Start:   55460 ms, End:   57084 ms\n",
      "2025-07-15 01:25:00 - INFO -   - Start:   57828 ms, End:   58876 ms\n",
      "2025-07-15 01:25:00 - INFO - 正在将 VAD 结果 (毫秒) 保存到文件: vocals_vad_timestamps_ms.json\n",
      "2025-07-15 01:25:00 - INFO - 开始根据VAD时间戳切割音频: vocals_16k_mono.wav\n",
      "2025-07-15 01:25:00 - INFO - 成功切割并保存了 393 个音频片段到目录: workspace/temp/vocals_vad_chunks\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# 3. 导入 VAD、音频处理和绘图所需的库\n",
    "# ===================================================================\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import json\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "from pydub import AudioSegment # 新增 pydub 导入\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value\n",
    "# ===================================================================\n",
    "# 步骤 3 的功能函数 (已更新)\n",
    "# ===================================================================\n",
    "\n",
    "def detect_speech_segments(audio_path: Path):\n",
    "    \"\"\"\n",
    "    【修正版】在音频上运行 Silero VAD 以检测语音片段。\n",
    "    使用经过参数搜索优化的最终参数。\n",
    "    \"\"\"\n",
    "    SAMPLING_RATE = 16000 # VAD 固定使用 16kHz\n",
    "    logging.info(\"正在加载 Silero VAD 模型...\")\n",
    "    try:\n",
    "        model, utils = torch.hub.load(\n",
    "            repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=False\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logging.error(f\"加载 Silero VAD 模型失败: {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    (get_speech_timestamps, _, read_audio, _, _) = utils\n",
    "    \n",
    "    logging.info(f\"正在读取音频文件用于 VAD: {audio_path.name}\")\n",
    "    wav_tensor = read_audio(str(audio_path), sampling_rate=SAMPLING_RATE)\n",
    "    \n",
    "    logging.info(\"开始使用 Silero VAD (优化参数) 检测语音时间戳...\")\n",
    "    # --- 使用最终确定的优化参数 ---\n",
    "    speech_timestamps_samples = get_speech_timestamps(\n",
    "        wav_tensor, model, sampling_rate=SAMPLING_RATE,\n",
    "        threshold=0.125, \n",
    "        min_silence_duration_ms=120,\n",
    "        min_speech_duration_ms=80, \n",
    "        speech_pad_ms=220\n",
    "    )\n",
    "    \n",
    "    # --- 将采样点转换为毫秒 ---\n",
    "    speech_timestamps_ms = []\n",
    "    for segment in speech_timestamps_samples:\n",
    "        start_ms = round(segment['start'] / (SAMPLING_RATE / 1000))\n",
    "        end_ms = round(segment['end'] / (SAMPLING_RATE / 1000))\n",
    "        speech_timestamps_ms.append({'start': start_ms, 'end': end_ms})\n",
    "        \n",
    "    logging.info(f\"VAD 检测到 {len(speech_timestamps_samples)} 个语音片段。\")\n",
    "    return speech_timestamps_samples, speech_timestamps_ms, model, utils\n",
    "\n",
    "def cut_audio_by_timestamps(original_audio_path: Path, timestamps_ms: list, output_chunk_dir: Path):\n",
    "    \"\"\"\n",
    "    【新增】根据VAD时间戳将音频切割成小块，并使用描述性名称保存。\n",
    "\n",
    "    参数:\n",
    "        original_audio_path (Path): 待切割的源音频文件路径 (16kHz, 单声道 .wav)。\n",
    "        timestamps_ms (list): 包含 {'start': ms, 'end': ms} 的列表。\n",
    "        output_chunk_dir (Path): 保存切割后音频块的目标目录。\n",
    "\n",
    "    返回:\n",
    "        Path: 创建的音频块目录路径。\n",
    "    \"\"\"\n",
    "    logging.info(f\"开始根据VAD时间戳切割音频: {original_audio_path.name}\")\n",
    "    \n",
    "    # 确保输出目录存在\n",
    "    output_chunk_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 使用 pydub 加载源音频\n",
    "    try:\n",
    "        source_audio = AudioSegment.from_wav(original_audio_path)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"使用pydub加载音频文件失败: {e}\")\n",
    "        return None\n",
    "\n",
    "    # 获取视频的基本名称，用于文件命名\n",
    "    video_stem = original_audio_path.stem.replace('_16k_mono', '')\n",
    "\n",
    "    for i, ts in enumerate(timestamps_ms):\n",
    "        start_ms = ts['start']\n",
    "        end_ms = ts['end']\n",
    "        \n",
    "        # 切割音频\n",
    "        audio_chunk = source_audio[start_ms:end_ms]\n",
    "        \n",
    "        # 构造具有高区分度的文件名\n",
    "        # 格式: S02E01_chunk_0001_2530ms_4710ms.wav\n",
    "        chunk_filename = f\"{video_stem}_chunk_{i+1:04d}_{start_ms}ms_{end_ms}ms.wav\"\n",
    "        chunk_path = output_chunk_dir / chunk_filename\n",
    "        \n",
    "        # 导出音频块\n",
    "        audio_chunk.export(chunk_path, format=\"wav\")\n",
    "        \n",
    "    logging.info(f\"成功切割并保存了 {len(timestamps_ms)} 个音频片段到目录: {output_chunk_dir}\")\n",
    "    return output_chunk_dir\n",
    "\n",
    "def plot_vad_results(audio_path: Path, speech_timestamps_samples: list):\n",
    "    \"\"\"\n",
    "    【无变动】使用采样点时间戳进行绘图，绘图文本为英文。\n",
    "    \"\"\"\n",
    "    # ... 此函数代码保持不变 ...\n",
    "    logging.info(\"正在生成 VAD 结果的可视化图表...\")\n",
    "    SAMPLING_RATE = 16000\n",
    "    try:\n",
    "        waveform, sr = librosa.load(str(audio_path), sr=SAMPLING_RATE)\n",
    "        time_axis = np.linspace(0, len(waveform) / sr, num=len(waveform))\n",
    "        vad_mask = np.zeros_like(waveform, dtype=np.int8)\n",
    "        for segment in speech_timestamps_samples:\n",
    "            vad_mask[segment['start']:segment['end']] = 1\n",
    "        plt.style.use('seaborn-v0_8-whitegrid')\n",
    "        fig, ax = plt.subplots(figsize=(200, 6), dpi=110)\n",
    "        ax.plot(time_axis, waveform, label='Audio Waveform', color='royalblue', linewidth=0.7)\n",
    "        y_min, y_max = ax.get_ylim()\n",
    "        ax.fill_between(time_axis, y_min, y_max, where=vad_mask==1, color='crimson', alpha=0.4, label='Voice Activity (VAD)')\n",
    "        ax.set_title(f\"VAD Speech Activity Result: {audio_path.name}\", fontsize=16)\n",
    "        ax.set_xlabel(\"Time (seconds)\"); ax.set_ylabel(\"Amplitude\")\n",
    "        ax.set_xlim(0, len(waveform) / sr); ax.legend(loc='upper right')\n",
    "        plt.tight_layout(); plt.show()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"生成 VAD 可视化图表时出错: {e}\\n{traceback.format_exc()}\")\n",
    "\n",
    "# ===================================================================\n",
    "# 测试区\n",
    "# ===================================================================\n",
    "logging.info(\"--- [测试开始] 步骤3：语音活动检测 (VAD) 与音频切割 ---\")\n",
    "\n",
    "if 'optimized_vocals_path' in locals() and optimized_vocals_path.exists():\n",
    "    logging.info(f\"将对文件进行 VAD 处理: {optimized_vocals_path.name}\")\n",
    "    \n",
    "    # --- 步骤 3.1: 运行 VAD 并获取两种格式的时间戳 ---\n",
    "    speech_timestamps_samples, speech_timestamps_ms, vad_model, vad_utils = detect_speech_segments(optimized_vocals_path)\n",
    "    \n",
    "    if speech_timestamps_ms:\n",
    "        logging.info(\"--- [VAD成功] VAD 处理完成 ---\")\n",
    "        logging.info(\"前5个检测到的语音片段 (单位: 毫秒):\")\n",
    "        for ts in speech_timestamps_ms[:5]:\n",
    "            logging.info(f\"  - Start: {ts['start']:>7d} ms, End: {ts['end']:>7d} ms\")\n",
    "        \n",
    "        # --- 步骤 3.2: 将【毫秒】时间戳保存到 JSON 文件 (用于存档和调试) ---\n",
    "        video_stem = optimized_vocals_path.stem.replace('_16k_mono', '')\n",
    "        vad_results_path = TEMP_DIR / f\"{video_stem}_vad_timestamps_ms.json\"\n",
    "        logging.info(f\"正在将 VAD 结果 (毫秒) 保存到文件: {vad_results_path.name}\")\n",
    "        with open(vad_results_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(speech_timestamps_ms, f, indent=2)\n",
    "\n",
    "        # --- 步骤 3.3 (新增): 根据时间戳切割音频 ---\n",
    "        # 定义切割后音频块的专用输出目录\n",
    "        vad_chunks_dir = TEMP_DIR / f\"{video_stem}_vad_chunks\"\n",
    "        # 调用新函数执行切割\n",
    "        cut_audio_by_timestamps(optimized_vocals_path, speech_timestamps_ms, vad_chunks_dir)\n",
    "            \n",
    "        # --- 步骤 3.4: 使用【采样点】时间戳进行精确绘图 (可选，用于可视化检查) ---\n",
    "        # plot_vad_results(optimized_vocals_path, speech_timestamps_samples)\n",
    "\n",
    "    else:\n",
    "        logging.warning(\"--- [测试警告] VAD 未能检测到任何语音片段 ---\")\n",
    "else:\n",
    "    logging.error(\"--- [测试失败] 找不到上一步生成的 'optimized_vocals_path' 文件。\")\n",
    "    logging.error(\"请确保上一个单元格已成功运行。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a9c2bd-f279-4b76-9901-826e506661d9",
   "metadata": {},
   "source": [
    "## 三.五 参数空间搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "056a03e6-81b5-48b1-9c6d-6f26bef6aefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===================================================================\n",
    "# # 6. 导入所需库 (已移除 concurrent.futures 和 os)\n",
    "# # ===================================================================\n",
    "# import logging\n",
    "# from pathlib import Path\n",
    "# import librosa\n",
    "# import json\n",
    "# import torch\n",
    "# import re\n",
    "# import numpy as np\n",
    "\n",
    "# # ===================================================================\n",
    "# # 辅助函数：解析SRT文件和计算IoU得分 (保持不变)\n",
    "# # ===================================================================\n",
    "\n",
    "# def _srt_time_to_ms(time_str: str) -> int:\n",
    "#     \"\"\"将 SRT 时间戳字符串 (HH:MM:SS,ms) 转换为总毫秒数。\"\"\"\n",
    "#     parts = re.split(r'[:,]', time_str)\n",
    "#     h, m, s, ms = map(int, parts)\n",
    "#     return h * 3600000 + m * 60000 + s * 1000 + ms\n",
    "\n",
    "# def parse_srt_file(srt_path: Path) -> list:\n",
    "#     \"\"\"解析SRT文件，返回一个包含语音片段起始和结束时间（毫秒）的列表。\"\"\"\n",
    "#     if not srt_path.exists():\n",
    "#         logging.error(f\"SRT 文件未找到: {srt_path}\")\n",
    "#         return []\n",
    "    \n",
    "#     segments = []\n",
    "#     try:\n",
    "#         with open(srt_path, 'r', encoding='utf-8-sig') as f:\n",
    "#             content = f.read()\n",
    "        \n",
    "#         pattern = re.compile(r'\\d+\\n(\\d{2}:\\d{2}:\\d{2},\\d{3}) --> (\\d{2}:\\d{2}:\\d{2},\\d{3})')\n",
    "#         matches = pattern.finditer(content)\n",
    "\n",
    "#         for match in matches:\n",
    "#             start_str, end_str = match.groups()\n",
    "#             segments.append({\n",
    "#                 'start': _srt_time_to_ms(start_str),\n",
    "#                 'end': _srt_time_to_ms(end_str)\n",
    "#             })\n",
    "#         logging.info(f\"成功从 '{srt_path.name}' 解析出 {len(segments)} 个字幕片段。\")\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"解析 SRT 文件 '{srt_path.name}' 时出错: {e}\")\n",
    "#         return []\n",
    "        \n",
    "#     return segments\n",
    "\n",
    "# def calculate_iou_score(ground_truth_segs: list, vad_segs: list, total_duration_ms: int) -> float:\n",
    "#     \"\"\"通过创建时间轴掩码来计算两组时间片段的 IoU (Intersection over Union) 得分。\"\"\"\n",
    "#     gt_timeline = np.zeros(total_duration_ms, dtype=bool)\n",
    "#     vad_timeline = np.zeros(total_duration_ms, dtype=bool)\n",
    "\n",
    "#     for seg in ground_truth_segs:\n",
    "#         gt_timeline[seg['start']:seg['end']] = True\n",
    "    \n",
    "#     for seg in vad_segs:\n",
    "#         vad_timeline[seg['start']:seg['end']] = True\n",
    "        \n",
    "#     intersection = np.sum(gt_timeline & vad_timeline)\n",
    "#     union = np.sum(gt_timeline | vad_timeline)\n",
    "\n",
    "#     if union == 0:\n",
    "#         return 1.0\n",
    "\n",
    "#     return intersection / union\n",
    "\n",
    "# # ===================================================================\n",
    "# # 定义参数空间和基准参数 (保持不变)\n",
    "# # ===================================================================\n",
    "# param_grid_single_test = {\n",
    "#     'threshold': [ 0.125, 0.1,0.15, 0.175, 0.20],\n",
    "#     'min_silence_duration_ms': [120, 140, 150, 160, 180],\n",
    "#     'min_speech_duration_ms': [60, 80, 100],\n",
    "#     'speech_pad_ms': [ 150, 180,200,220,250]\n",
    "# }\n",
    "# base_params = {\n",
    "#     'threshold': 0.125, 'min_silence_duration_ms': 120,\n",
    "#     'min_speech_duration_ms': 80, 'speech_pad_ms': 220\n",
    "# }\n",
    "\n",
    "# # ===================================================================\n",
    "# # 单进程串行测试主程序 (已移除多进程逻辑)\n",
    "# # ===================================================================\n",
    "# def run_serial_vad_testing(audio_for_tuning_path: Path, srt_ground_truth_path: Path, \n",
    "#                            vad_model, vad_utils):\n",
    "#     \"\"\"\n",
    "#     【单进程串行版】主测试函数。\n",
    "#     \"\"\"\n",
    "#     logging.info(\"--- [开始] VAD参数单变量影响测试 (单进程串行版) ---\")\n",
    "#     print(f\"将使用以下基准参数进行对照: {base_params}\")\n",
    "    \n",
    "#     # --- 步骤1: 检查并加载数据 ---\n",
    "#     if not audio_for_tuning_path.exists() or not srt_ground_truth_path.exists():\n",
    "#         logging.error(\"音频或SRT文件路径无效，测试中止。\")\n",
    "#         return\n",
    "\n",
    "#     ground_truth_segments = parse_srt_file(srt_ground_truth_path)\n",
    "#     if not ground_truth_segments:\n",
    "#         logging.error(\"无法加载SRT文件，测试中止。\")\n",
    "#         return\n",
    "\n",
    "#     duration_sec = librosa.get_duration(path=str(audio_for_tuning_path))\n",
    "#     audio_duration_ms = int(duration_sec * 1000)\n",
    "    \n",
    "#     (get_speech_timestamps, _, read_audio, _, _) = vad_utils\n",
    "#     wav_tensor = read_audio(str(audio_for_tuning_path), sampling_rate=16000)\n",
    "    \n",
    "#     all_single_test_results = {}\n",
    "    \n",
    "#     # --- 步骤2: 循环测试每个参数 ---\n",
    "#     for param_key, param_values in param_grid_single_test.items():\n",
    "#         print(f\"\\n--- 正在测试参数: '{param_key}' ---\")\n",
    "        \n",
    "#         current_param_results = []\n",
    "        \n",
    "#         # --- 步骤3: 使用简单的 for 循环串行执行 ---\n",
    "#         for value in param_values:\n",
    "#             # 准备本次测试的参数\n",
    "#             test_params = base_params.copy()\n",
    "#             test_params[param_key] = value\n",
    "            \n",
    "#             # 执行VAD检测\n",
    "#             speech_timestamps_samples = get_speech_timestamps(\n",
    "#                 wav_tensor, vad_model, sampling_rate=16000, **test_params\n",
    "#             )\n",
    "            \n",
    "#             # 计算结果\n",
    "#             vad_segments_ms = [\n",
    "#                 {'start': round(s['start'] / 16), 'end': round(s['end'] / 16)} \n",
    "#                 for s in speech_timestamps_samples\n",
    "#             ]\n",
    "#             iou_score = calculate_iou_score(\n",
    "#                 ground_truth_segments, vad_segments_ms, audio_duration_ms\n",
    "#             )\n",
    "            \n",
    "#             # 打印单次结果\n",
    "#             num_srt_segs = len(ground_truth_segments)\n",
    "#             segs = len(vad_segments_ms)\n",
    "#             print(f\"  > 当 {param_key:<25} = {value:<5} | IoU得分: {iou_score:.4f} (片段数: {segs} / 标准: {num_srt_segs})\")\n",
    "\n",
    "#             # 保存单次结果\n",
    "#             current_param_results.append({\n",
    "#                 'params': test_params, 'iou_score': iou_score,\n",
    "#                 'num_vad_segments': segs\n",
    "#             })\n",
    "\n",
    "#         all_single_test_results[param_key] = current_param_results\n",
    "#         print(\"-\" * 75)\n",
    "\n",
    "#     # --- 步骤4: 保存最终结果 ---\n",
    "#     results_path = WORKSPACE_DIR / \"vad_serial_tuning_results.json\"\n",
    "#     with open(results_path, 'w', encoding='utf-8') as f:\n",
    "#         json.dump(all_single_test_results, f, indent=4, ensure_ascii=False)\n",
    "        \n",
    "#     logging.info(f\"--- [完成] 所有串行测试结果已保存到: {results_path} ---\")\n",
    "\n",
    "\n",
    "# # ===================================================================\n",
    "# # --- 调用主函数 (与原始版本相同) ---\n",
    "# # ===================================================================\n",
    "# # 1. 定义全局变量 (这些是在其他单元格中创建的)\n",
    "# SRT_GROUND_TRUTH_PATH = Path(\"E01_cn.srt\") \n",
    "\n",
    "# # 2. 检查全局变量是否存在，再调用函数\n",
    "# if 'optimized_vocals_path' in locals() and 'vad_model' in locals() and 'vad_utils' in locals():\n",
    "#     # 3. 将全局变量作为参数，传入函数中\n",
    "#     run_serial_vad_testing(\n",
    "#         audio_for_tuning_path=optimized_vocals_path,\n",
    "#         srt_ground_truth_path=SRT_GROUND_TRUTH_PATH,\n",
    "#         vad_model=vad_model,\n",
    "#         vad_utils=vad_utils\n",
    "#     )\n",
    "# else:\n",
    "#     logging.error(\"无法启动测试：一个或多个必需的全局变量 (optimized_vocals_path, vad_model, vad_utils) 未定义。请确保之前的单元格已成功运行。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f359a5df-8dc8-406a-84f4-4fa8e58e65be",
   "metadata": {},
   "source": [
    "## 四 ASR&原文字幕生成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e6299b-8d90-4fba-9bcc-646bf5a5d33f",
   "metadata": {},
   "source": [
    "#### 4.1 使用VAD切分的音频进行识别(首选)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eba35973-dbb5-4d11-9902-a21ef259b2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-15 01:47:09 - INFO - --- [测试开始] 步骤4：ASR语音识别 (使用 OpenAI-Whisper) ---\n",
      "2025-07-15 01:47:09 - INFO - 正在加载官方 Whisper 模型: 'large-v3'\n",
      "2025-07-15 01:47:09 - INFO - 将使用设备: cuda\n",
      "2025-07-15 01:47:25 - INFO - ✅ 官方 Whisper 模型加载成功。\n",
      "2025-07-15 01:47:25 - INFO - 准备从目录 'vocals_vad_chunks' 中逐个识别音频片段...\n",
      "2025-07-15 01:47:25 - INFO - 找到 393 个音频片段，开始逐个进行语音识别...\n",
      "2025-07-15 01:52:01 - INFO - ✅ 完成所有片段的识别，共获得 376 条有效转录。\n",
      "2025-07-15 01:52:01 - INFO - 正在生成 SRT 字幕文件到: workspace/output/vocals_ja.srt\n",
      "2025-07-15 01:52:01 - INFO - ✅ 日语 SRT 字幕文件已成功保存。\n",
      "2025-07-15 01:52:01 - INFO - \n",
      " --- [测试成功] 步骤4全部完成 ---\n",
      "2025-07-15 01:52:01 - INFO - 生成的日语字幕文件预览 (前5条):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:00:00,868 --> 00:00:01,724] ご視聴ありがとうございました\n",
      "[00:00:47,236 --> 00:00:49,724] やっぱり海はいいなあ\n",
      "[00:00:51,844 --> 00:00:52,636] おっ\n",
      "[00:00:55,460 --> 00:00:57,084] しおりちゃんから\n",
      "[00:00:57,828 --> 00:00:58,876] しおり?\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# 4. 导入 ASR 和字幕生成所需的库 (已切换到 OpenAI-Whisper)\n",
    "# ===================================================================\n",
    "# from faster_whisper import WhisperModel  <- 已移除\n",
    "import whisper  # <--- 使用官方的 whisper 库\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# ===================================================================\n",
    "# 步骤 4 的功能函数 (已适配 OpenAI-Whisper)\n",
    "# ===================================================================\n",
    "\n",
    "def load_original_whisper_model():\n",
    "    \"\"\"\n",
    "    【新】加载官方的 OpenAI-Whisper 模型。\n",
    "    它会自动处理模型的下载和缓存。\n",
    "    \"\"\"\n",
    "    # 'large-v3' 是当前最先进的模型\n",
    "    model_name = \"large-v3\" \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    logging.info(f\"正在加载官方 Whisper 模型: '{model_name}'\")\n",
    "    logging.info(f\"将使用设备: {device}\")\n",
    "    \n",
    "    try:\n",
    "        # load_model 会自动下载并缓存模型，非常稳定\n",
    "        model = whisper.load_model(model_name, device=device)\n",
    "        logging.info(\"✅ 官方 Whisper 模型加载成功。\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logging.error(f\"❌ 加载官方 Whisper 模型失败: {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "def format_time_for_srt(milliseconds: int) -> str:\n",
    "    \"\"\"将毫秒转换为 SRT 的 'HH:MM:SS,ms' 格式。(此函数无需改变)\"\"\"\n",
    "    td = timedelta(milliseconds=milliseconds)\n",
    "    hours, remainder = divmod(td.seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    millis = td.microseconds // 1000\n",
    "    return f\"{hours:02d}:{minutes:02d}:{seconds:02d},{millis:03d}\"\n",
    "\n",
    "def transcribe_audio_chunks_original(whisper_model, audio_chunks_dir: Path):\n",
    "    \"\"\"\n",
    "    【新】使用官方 Whisper 模型，通过循环逐个处理音频文件。\n",
    "    同样加入了内存管理和错误捕获。\n",
    "    \"\"\"\n",
    "    logging.info(f\"准备从目录 '{audio_chunks_dir.name}' 中逐个识别音频片段...\")\n",
    "    \n",
    "    chunk_paths = sorted(audio_chunks_dir.glob(\"*.wav\"))\n",
    "    if not chunk_paths:\n",
    "        logging.warning(f\"在指定目录中未找到任何 .wav 音频片段。\")\n",
    "        return []\n",
    "\n",
    "    logging.info(f\"找到 {len(chunk_paths)} 个音频片段，开始逐个进行语音识别...\")\n",
    "    \n",
    "    transcription_results = []\n",
    "    total_chunks = len(chunk_paths)\n",
    "    \n",
    "    for i, chunk_path in enumerate(chunk_paths):\n",
    "        try:\n",
    "            # logging.info(f\"--> 正在处理片段 [{i+1}/{total_chunks}]: {chunk_path.name}\")\n",
    "            \n",
    "            chunk_path_str = str(chunk_path)\n",
    "            \n",
    "            # --- 【核心修改】调用官方 whisper 的 transcribe 方法 ---\n",
    "            # 它返回的是一个包含所有信息的字典，而不是生成器\n",
    "            result = whisper_model.transcribe(\n",
    "                chunk_path_str,\n",
    "                language=\"ja\",      # 指定语言\n",
    "                fp16=torch.cuda.is_available() # 在GPU上自动使用fp16\n",
    "            )\n",
    "            \n",
    "            # 从结果字典中获取识别出的文本\n",
    "            text = result[\"text\"].strip()\n",
    "\n",
    "            if text:\n",
    "                try:\n",
    "                    # 从文件名解析时间戳的逻辑保持不变\n",
    "                    parts = chunk_path.stem.split('_')\n",
    "                    start_ms = int(parts[-2].replace('ms', ''))\n",
    "                    end_ms = int(parts[-1].replace('ms', ''))\n",
    "                    \n",
    "                    transcription_results.append({\n",
    "                        \"start_ms\": start_ms,\n",
    "                        \"end_ms\": end_ms,\n",
    "                        \"text\": text\n",
    "                    })\n",
    "                except (IndexError, ValueError) as e:\n",
    "                    logging.warning(f\"无法从文件名 {chunk_path.name} 解析时间戳: {e}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"❌ 处理文件 {chunk_path.name} 时发生严重错误，已跳过。\", exc_info=True)\n",
    "        \n",
    "        finally:\n",
    "            # 内存管理措施依然保留，这是一个好习惯\n",
    "            gc.collect() \n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    logging.info(f\"✅ 完成所有片段的识别，共获得 {len(transcription_results)} 条有效转录。\")\n",
    "    return transcription_results\n",
    "    \n",
    "def generate_srt_from_transcriptions(transcriptions: list, srt_output_path: Path):\n",
    "    \"\"\"\n",
    "    将转录结果列表生成为标准的 SRT 字幕文件。(修正版)\n",
    "    直接逐行写入文件，以彻底避免字符串转义问题。\n",
    "    \"\"\"\n",
    "    logging.info(f\"正在生成 SRT 字幕文件到: {srt_output_path}\")\n",
    "    transcriptions.sort(key=lambda x: x['start_ms'])\n",
    "    \n",
    "    try:\n",
    "        with open(srt_output_path, 'w', encoding='utf-8') as f:\n",
    "            for i, res in enumerate(transcriptions, start=1):\n",
    "                start_time_str = format_time_for_srt(res['start_ms'])\n",
    "                end_time_str = format_time_for_srt(res['end_ms'])\n",
    "                text = res['text']\n",
    "                \n",
    "                # 逐行写入文件，确保换行符被正确处理\n",
    "                f.write(f\"{i}\\n\")\n",
    "                f.write(f\"{start_time_str} --> {end_time_str}\\n\")\n",
    "                # 在文本末尾写入两个换行符，一个用于文本本身换行，一个用于和下一个字幕块隔开\n",
    "                f.write(f\"{text}\\n\\n\")\n",
    "                \n",
    "        logging.info(f\"✅ 日语 SRT 字幕文件已成功保存。\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"❌ 保存 SRT 文件时出错: {e}\", exc_info=True)\n",
    "\n",
    "# ===================================================================\n",
    "# 测试区 (已适配新函数)\n",
    "# ===================================================================\n",
    "logging.info(\"--- [测试开始] 步骤4：ASR语音识别 (使用 OpenAI-Whisper) ---\")\n",
    "\n",
    "if 'vad_chunks_dir' in locals() and vad_chunks_dir.exists():\n",
    "    \n",
    "    # 【修改】调用新的模型加载函数\n",
    "    whisper_model = load_original_whisper_model()\n",
    "    \n",
    "    if whisper_model:\n",
    "        # 【修改】调用新的转录函数\n",
    "        transcription_data = transcribe_audio_chunks_original(whisper_model, vad_chunks_dir)\n",
    "        \n",
    "        if transcription_data:\n",
    "            video_stem = vad_chunks_dir.name.replace('_vad_chunks', '')\n",
    "            japanese_srt_path = OUTPUT_DIR / f\"{video_stem}_ja.srt\"\n",
    "            \n",
    "            generate_srt_from_transcriptions(transcription_data, japanese_srt_path)\n",
    "            \n",
    "            logging.info(\"\\n --- [测试成功] 步骤4全部完成 ---\")\n",
    "            logging.info(\"生成的日语字幕文件预览 (前5条):\")\n",
    "            for item in transcription_data[:5]:\n",
    "                start = format_time_for_srt(item['start_ms'])\n",
    "                end = format_time_for_srt(item['end_ms'])\n",
    "                print(f\"[{start} --> {end}] {item['text']}\")\n",
    "        else:\n",
    "            logging.warning(\"--- [测试警告] ASR 未能从音频片段中识别出任何文本。---\")\n",
    "else:\n",
    "    logging.error(\"--- [测试失败] 找不到上一步生成的 'vad_chunks_dir' 目录。\")\n",
    "    logging.error(\"请确保上一个单元格已成功运行。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6995240-e731-4539-bb67-b6401e5c30f2",
   "metadata": {},
   "source": [
    "#### 4.x batch加速版（并非加速，弃用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c815b4-6731-479f-b2a8-ddcbf8557c97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # ===================================================================\n",
    "# # 4. 导入 ASR 和字幕生成所需的库\n",
    "# # ===================================================================\n",
    "# import logging\n",
    "# from pathlib import Path\n",
    "# from datetime import timedelta\n",
    "# import torch\n",
    "# import gc\n",
    "# import os\n",
    "# from transformers import pipeline\n",
    "# from transformers.utils import is_flash_attn_2_available # 引入 Flash Attention 2 的检查工具\n",
    "# import warnings\n",
    "# warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# # ===================================================================\n",
    "# # 步骤 4 的功能函数\n",
    "# # ===================================================================\n",
    "\n",
    "# def load_optimized_whisper_pipeline():\n",
    "#     \"\"\"\n",
    "#     加载经过 Flash Attention 2 优化的 Hugging Face ASR pipeline。\n",
    "#     \"\"\"\n",
    "#     model_name = \"openai/whisper-large-v3\"\n",
    "#     device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "#     torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "#     logging.info(f\"正在加载优化的 ASR pipeline: '{model_name}'\")\n",
    "#     logging.info(f\"将使用设备: {device}, 计算类型: {torch_dtype}\")\n",
    "\n",
    "#     if is_flash_attn_2_available():\n",
    "#         attn_implementation = \"flash_attention_2\"\n",
    "#         logging.info(\"✅ Flash Attention 2 可用，将启用以获得最大加速！\")\n",
    "#     else:\n",
    "#         attn_implementation = \"sdpa\" # Scaled Dot Product Attention，PyTorch 2.0+ 的内置高效实现\n",
    "#         logging.info(\"⚠️ Flash Attention 2 不可用，将使用 PyTorch 内置的 SDPA。性能依然很好。\")\n",
    "        \n",
    "#     try:\n",
    "#         # 直接使用 Hugging Face pipeline，并传入优化参数\n",
    "#         asr_pipeline = pipeline(\n",
    "#             \"automatic-speech-recognition\",\n",
    "#             model=model_name,\n",
    "#             torch_dtype=torch_dtype,\n",
    "#             device=device,\n",
    "#             model_kwargs={\"attn_implementation\": attn_implementation}\n",
    "#         )\n",
    "#         logging.info(\"✅ 优化后的 ASR pipeline 加载成功。\")\n",
    "#         return asr_pipeline\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"❌ 加载 ASR pipeline 失败: {e}\", exc_info=True)\n",
    "#         return None\n",
    "\n",
    "# def format_time_for_srt(milliseconds: int) -> str:\n",
    "#     \"\"\"将毫秒转换为 SRT 的 'HH:MM:SS,ms' 格式。\"\"\"\n",
    "#     td = timedelta(milliseconds=milliseconds)\n",
    "#     hours, remainder = divmod(td.seconds, 3600)\n",
    "#     minutes, seconds = divmod(remainder, 60)\n",
    "#     millis = td.microseconds // 1000\n",
    "#     return f\"{hours:02d}:{minutes:02d}:{seconds:02d},{millis:03d}\"\n",
    "\n",
    "# def transcribe_chunks_with_pipeline(asr_pipeline, audio_chunks_dir: Path, batch_size: int):\n",
    "#     \"\"\"\n",
    "#     使用优化后的 pipeline 和批处理功能高效转录所有音频块。\n",
    "#     \"\"\"\n",
    "#     logging.info(f\"准备从目录 '{audio_chunks_dir.name}' 中进行批处理识别...\")\n",
    "    \n",
    "#     chunk_paths = [str(p) for p in sorted(audio_chunks_dir.glob(\"*.wav\"))]\n",
    "#     if not chunk_paths:\n",
    "#         logging.warning(f\"在指定目录中未找到任何 .wav 音频片段。\")\n",
    "#         return []\n",
    "\n",
    "#     logging.info(f\"找到 {len(chunk_paths)} 个音频片段，开始使用 batch_size={batch_size} 进行批处理...\")\n",
    "    \n",
    "#     transcription_results = []\n",
    "    \n",
    "#     try:\n",
    "#         # --- 直接调用 pipeline，它原生支持批处理 ---\n",
    "#         # `pipeline` 可以直接处理一个文件路径列表\n",
    "#         outputs = asr_pipeline(\n",
    "#             chunk_paths,\n",
    "#             chunk_length_s=10, # 这是 whisper 的标准块长度\n",
    "#             batch_size=batch_size,\n",
    "#             generate_kwargs={\"language\": \"japanese\"} # 指定语言\n",
    "#         )\n",
    "        \n",
    "#         logging.info(f\"✅ 所有批次处理完成。正在整理 {len(outputs)} 条结果...\")\n",
    "\n",
    "#         # 将结果与原始文件路径对应起来，以提取时间戳\n",
    "#         for i, output in enumerate(outputs):\n",
    "#             text = output[\"text\"].strip()\n",
    "#             if text:\n",
    "#                 try:\n",
    "#                     chunk_path = Path(chunk_paths[i])\n",
    "#                     parts = chunk_path.stem.split('_')\n",
    "#                     start_ms = int(parts[-2].replace('ms', ''))\n",
    "#                     end_ms = int(parts[-1].replace('ms', ''))\n",
    "                    \n",
    "#                     transcription_results.append({\n",
    "#                         \"start_ms\": start_ms,\n",
    "#                         \"end_ms\": end_ms,\n",
    "#                         \"text\": text\n",
    "#                     })\n",
    "#                 except (IndexError, ValueError) as e:\n",
    "#                     logging.warning(f\"无法从文件名 {chunk_path.name} 解析时间戳: {e}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"❌ 在批处理过程中发生严重错误。\", exc_info=True)\n",
    "    \n",
    "#     finally:\n",
    "#         gc.collect()\n",
    "#         if torch.cuda.is_available():\n",
    "#             torch.cuda.empty_cache()\n",
    "\n",
    "#     logging.info(f\"成功整理出 {len(transcription_results)} 条有效转录。\")\n",
    "#     return transcription_results\n",
    "\n",
    "# def generate_srt_from_transcriptions(transcriptions: list, srt_output_path: Path):\n",
    "#     \"\"\"将转录结果列表生成为标准的 SRT 字幕文件。\"\"\"\n",
    "#     logging.info(f\"正在生成 SRT 字幕文件到: {srt_output_path}\")\n",
    "#     srt_content = []\n",
    "#     transcriptions.sort(key=lambda x: x['start_ms'])\n",
    "    \n",
    "#     for i, res in enumerate(transcriptions, start=1):\n",
    "#         start_time_str = format_time_for_srt(res['start_ms'])\n",
    "#         end_time_str = format_time_for_srt(res['end_ms'])\n",
    "#         text = res['text']\n",
    "#         srt_block = f\"{i}\\\\n{start_time_str} --> {end_time_str}\\\\n{text}\\\\n\"\n",
    "#         srt_content.append(srt_block)\n",
    "        \n",
    "#     final_srt = \"\\\\n\".join(srt_content)\n",
    "    \n",
    "#     try:\n",
    "#         with open(srt_output_path, 'w', encoding='utf-8') as f:\n",
    "#             f.write(final_srt)\n",
    "#         logging.info(f\"✅ 日语 SRT 字幕文件已成功保存。\")\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"❌ 保存 SRT 文件时出错: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "# # ===================================================================\n",
    "# # 测试区 \n",
    "# # ===================================================================\n",
    "# logging.info(\"--- [测试开始] 步骤4：ASR语音识别---\")\n",
    "\n",
    "# if 'vad_chunks_dir' in locals() and vad_chunks_dir.exists():\n",
    "    \n",
    "#     # 加载我们新的、优化过的 pipeline\n",
    "#     asr_pipeline = load_optimized_whisper_pipeline()\n",
    "    \n",
    "#     if asr_pipeline:\n",
    "#         # --- 定义批处理大小 ---\n",
    "#         # 对于 3090 (24GB VRAM)，Flash Attention 2 更节省显存，可以尝试更大的批次\n",
    "#         BATCH_SIZE = 8\n",
    "        \n",
    "#         transcription_data = transcribe_chunks_with_pipeline(asr_pipeline, vad_chunks_dir, BATCH_SIZE)\n",
    "        \n",
    "#         if transcription_data:\n",
    "#             video_stem = vad_chunks_dir.name.replace('_vad_chunks', '')\n",
    "#             japanese_srt_path = OUTPUT_DIR / f\"{video_stem}_ja.srt\"\n",
    "            \n",
    "#             generate_srt_from_transcriptions(transcription_data, japanese_srt_path)\n",
    "            \n",
    "#             logging.info(\"\\\\n--- [测试成功] 步骤4全部完成 ---\")\n",
    "#             logging.info(\"生成的日语字幕文件预览 (前5条):\")\n",
    "#             for item in transcription_data[:5]:\n",
    "#                 start = format_time_for_srt(item['start_ms'])\n",
    "#                 end = format_time_for_srt(item['end_ms'])\n",
    "#                 print(f\"[{start} --> {end}] {item['text']}\")\n",
    "#         else:\n",
    "#             logging.warning(\"--- [测试警告] ASR 未能从音频片段中识别出任何文本。---\")\n",
    "# else:\n",
    "#     logging.error(\"--- [测试失败] 找不到上一步生成的 'vad_chunks_dir' 目录。\")\n",
    "#     logging.error(\"请确保上一个单元格已成功运行。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbe7c7c-1425-4949-937c-0884b8fdc99d",
   "metadata": {},
   "source": [
    "#### 4.2 混合策略(flag,To be improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec99104-0d8e-4395-bf6b-21d0de78465e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# 4. 导入 ASR 和字幕生成所需的库 (终极混合策略版)\n",
    "# ===================================================================\n",
    "import whisper\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "import json # 需要 json 库来加载 VAD 时间戳\n",
    "\n",
    "def load_original_whisper_model():\n",
    "    \"\"\"\n",
    "    【新】加载官方的 OpenAI-Whisper 模型。\n",
    "    它会自动处理模型的下载和缓存。\n",
    "    \"\"\"\n",
    "    # 'large-v3' 是当前最先进的模型\n",
    "    model_name = \"large-v3\" \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    logging.info(f\"正在加载官方 Whisper 模型: '{model_name}'\")\n",
    "    logging.info(f\"将使用设备: {device}\")\n",
    "    \n",
    "    try:\n",
    "        # load_model 会自动下载并缓存模型，非常稳定\n",
    "        model = whisper.load_model(model_name, device=device)\n",
    "        logging.info(\"✅ 官方 Whisper 模型加载成功。\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logging.error(f\"❌ 加载官方 Whisper 模型失败: {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "def format_time_for_srt(milliseconds: int) -> str:\n",
    "    \"\"\"将毫秒转换为 SRT 的 'HH:MM:SS,ms' 格式。(此函数无需改变)\"\"\"\n",
    "    td = timedelta(milliseconds=milliseconds)\n",
    "    hours, remainder = divmod(td.seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    millis = td.microseconds // 1000\n",
    "    return f\"{hours:02d}:{minutes:02d}:{seconds:02d},{millis:03d}\"\n",
    "\n",
    "# ===================================================================\n",
    "# 【核心】新的功能函数\n",
    "# ===================================================================\n",
    "def transcribe_long_audio_with_vad_guidance(whisper_model, long_audio_path: Path, vad_timestamps_path: Path):\n",
    "    \"\"\"\n",
    "    【新】使用 Whisper 处理长音频，并用外部 VAD 的结果来过滤和校准。\n",
    "    \"\"\"\n",
    "    logging.info(f\"准备使用 Whisper 处理长音频: {long_audio_path.name}\")\n",
    "    logging.info(f\"并使用 VAD 时间戳进行指导: {vad_timestamps_path.name}\")\n",
    "\n",
    "    if not long_audio_path.exists() or not vad_timestamps_path.exists():\n",
    "        logging.error(f\"音频文件或 VAD 时间戳文件不存在。\")\n",
    "        return []\n",
    "\n",
    "    # 1. 加载我们精确的 Silero-VAD 时间戳 (单位：秒)\n",
    "    with open(vad_timestamps_path, 'r') as f:\n",
    "        # 将毫秒转换为秒\n",
    "        vad_timestamps = [{'start': ts['start']/1000.0, 'end': ts['end']/1000.0} for ts in json.load(f)]\n",
    "\n",
    "    # 2. 调用 Whisper 进行转录，并设置宽松的参数\n",
    "    logging.info(\"开始使用 Whisper 进行初步转录，参数已设置为宽松模式...\")\n",
    "    try:\n",
    "        # no_speech_threshold: 设低一点，让 Whisper 更不容易将音频判断为无语音\n",
    "        # condition_on_previous_text: 设为 False 可能有助于减少长静音后的重复或幻觉\n",
    "        result = whisper_model.transcribe(\n",
    "            str(long_audio_path),\n",
    "            language=\"ja\",\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            no_speech_threshold=0.5, # 默认是 0.6，稍微降低\n",
    "            condition_on_previous_text=False \n",
    "        )\n",
    "        whisper_segments = result['segments']\n",
    "        logging.info(f\"Whisper 初步转录完成，得到 {len(whisper_segments)} 个片段。\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"❌ Whisper 转录过程中发生严重错误。\", exc_info=True)\n",
    "        return []\n",
    "\n",
    "    # 3. 【关键】用 VAD 时间戳过滤和对齐 Whisper 的结果\n",
    "    final_segments = []\n",
    "    logging.info(\"开始使用 Silero-VAD 时间戳对 Whisper 结果进行校准...\")\n",
    "\n",
    "    for whisper_seg in whisper_segments:\n",
    "        # 检查这个 Whisper 片段的时间范围是否与任何 VAD 片段有重叠\n",
    "        is_valid_segment = False\n",
    "        for vad_ts in vad_timestamps:\n",
    "            # 计算重叠部分\n",
    "            overlap_start = max(whisper_seg['start'], vad_ts['start'])\n",
    "            overlap_end = min(whisper_seg['end'], vad_ts['end'])\n",
    "            \n",
    "            # 如果重叠时长大于 0 (或一个很小的阈值)，就认为这个片段是有效的\n",
    "            if overlap_end > overlap_start:\n",
    "                is_valid_segment = True\n",
    "                break # 只要跟一个 VAD 片段有重叠就行\n",
    "        \n",
    "        if is_valid_segment:\n",
    "            final_segments.append(whisper_seg)\n",
    "        else:\n",
    "            logging.warning(f\"丢弃 Whisper 片段 (可能为幻觉): [{whisper_seg['start']:.2f}s -> {whisper_seg['end']:.2f}s] {whisper_seg['text']}\")\n",
    "\n",
    "    logging.info(f\"校准完成。最终保留 {len(final_segments)} / {len(whisper_segments)} 个片段。\")\n",
    "    \n",
    "    # 清理内存\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    return final_segments\n",
    "\n",
    "def generate_srt_from_transcriptions(transcriptions: list, srt_output_path: Path):\n",
    "    \"\"\"将转录结果列表生成为标准的 SRT 字幕文件。(此函数无需改变)\"\"\"\n",
    "    logging.info(f\"正在生成 SRT 字幕文件到: {srt_output_path}\")\n",
    "    srt_content = []\n",
    "    transcriptions.sort(key=lambda x: x['start_ms'])\n",
    "    \n",
    "    for i, res in enumerate(transcriptions, start=1):\n",
    "        start_time_str = format_time_for_srt(res['start_ms'])\n",
    "        end_time_str = format_time_for_srt(res['end_ms'])\n",
    "        text = res['text']\n",
    "        srt_block = f\"{i}\\\\n{start_time_str} --> {end_time_str}\\\\n{text}\\\\n\"\n",
    "        srt_content.append(srt_block)\n",
    "        \n",
    "    final_srt = \"\\\\n\".join(srt_content)\n",
    "    \n",
    "    try:\n",
    "        with open(srt_output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(final_srt)\n",
    "        logging.info(f\"✅ 日语 SRT 字幕文件已成功保存。\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"❌ 保存 SRT 文件时出错: {e}\", exc_info=True)\n",
    "\n",
    "# ===================================================================\n",
    "# 测试区 (已适配混合策略)\n",
    "# ===================================================================\n",
    "logging.info(\"--- [测试开始] 步骤4：ASR语音识别 (使用终极混合策略) ---\")\n",
    "\n",
    "try:\n",
    "    video_stem = \"vocals\" # 请根据实际情况修改\n",
    "    # 需要两个输入文件：\n",
    "    vocals_16k_mono_path = TEMP_DIR / f\"{video_stem}_16k_mono.wav\"\n",
    "    vad_timestamps_path = TEMP_DIR / f\"{video_stem}_vad_timestamps_ms.json\"\n",
    "\n",
    "    if vocals_16k_mono_path.exists() and vad_timestamps_path.exists():\n",
    "        whisper_model = load_original_whisper_model()\n",
    "        \n",
    "        if whisper_model:\n",
    "            # 调用我们新的混合策略函数\n",
    "            final_transcription_segments = transcribe_long_audio_with_vad_guidance(\n",
    "                whisper_model, \n",
    "                vocals_16k_mono_path, \n",
    "                vad_timestamps_path\n",
    "            )\n",
    "            \n",
    "            if final_transcription_segments:\n",
    "                japanese_srt_path = OUTPUT_DIR / f\"{video_stem}_ja.srt\"\n",
    "                generate_srt_from_segments(final_transcription_segments, japanese_srt_path)\n",
    "                \n",
    "                logging.info(\"\\\\n--- [测试成功] 步骤4全部完成 ---\")\n",
    "                logging.info(\"生成的日语字幕文件预览 (前5条):\")\n",
    "                for item in transcription_data[:5]:\n",
    "                    start = format_time_for_srt(item['start_ms'])\n",
    "                    end = format_time_for_srt(item['end_ms'])\n",
    "                    print(f\"[{start} --> {end}] {item['text']}\")\n",
    "            else:\n",
    "                logging.warning(\"--- [测试警告] ASR 未能从音频中识别出任何有效文本。---\")\n",
    "    else:\n",
    "        logging.error(f\"--- [测试失败] 缺少所需文件: {vocals_16k_mono_path} 或 {vad_timestamps_path}\")\n",
    "\n",
    "except NameError as e:\n",
    "    logging.error(f\"--- [测试失败] 变量未定义: {e}。请确保前面的单元格已成功运行。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e428fbf1-6534-4026-ab0b-e494c93ac096",
   "metadata": {},
   "source": [
    "#### 4.3 纯whisper(切分不准确)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a8bc6d-1ab7-4f91-b4ec-6104c11d0bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# 4. 导入 ASR 和字幕生成所需的库 (已切换到 OpenAI-Whisper，简化版)\n",
    "# ===================================================================\n",
    "import whisper\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# ===================================================================\n",
    "# 步骤 4 的功能函数 (已适配 OpenAI-Whisper，简化版)\n",
    "# ===================================================================\n",
    "\n",
    "def load_original_whisper_model():\n",
    "    \"\"\"\n",
    "    加载官方的 OpenAI-Whisper 模型。\n",
    "    它会自动处理模型的下载和缓存。\n",
    "    \"\"\"\n",
    "    model_name = \"large-v3\" \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    logging.info(f\"正在加载官方 Whisper 模型: '{model_name}'\")\n",
    "    logging.info(f\"将使用设备: {device}\")\n",
    "    \n",
    "    try:\n",
    "        model = whisper.load_model(model_name, device=device)\n",
    "        logging.info(\"✅ 官方 Whisper 模型加载成功。\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logging.error(f\"❌ 加载官方 Whisper 模型失败: {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "def format_time_for_srt(seconds: float) -> str:\n",
    "    \"\"\"将秒数转换为 SRT 的 'HH:MM:SS,ms' 格式。\"\"\"\n",
    "    td = timedelta(seconds=seconds)\n",
    "    # 使用 total_seconds() 来正确处理大于一天的情况（虽然这里不需要）\n",
    "    # 但更简洁的写法是直接从 timedelta 对象中提取\n",
    "    millis = td.microseconds // 1000\n",
    "    total_seconds = int(td.total_seconds())\n",
    "    hours, remainder = divmod(total_seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    return f\"{hours:02d}:{minutes:02d}:{seconds:02d},{millis:03d}\"\n",
    "\n",
    "def transcribe_long_audio_with_whisper(whisper_model, long_audio_path: Path):\n",
    "    \"\"\"\n",
    "    【新】使用官方 Whisper 模型，直接处理一个长音频文件。\n",
    "    Whisper 会在内部自动分块并返回带时间戳的片段。\n",
    "    \"\"\"\n",
    "    logging.info(f\"准备使用 Whisper 直接处理长音频文件: {long_audio_path.name}\")\n",
    "    \n",
    "    if not long_audio_path.exists():\n",
    "        logging.error(f\"音频文件不存在: {long_audio_path}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        logging.info(\"开始使用 Whisper 进行转录，参数已设置为宽松模式...\")\n",
    "        # no_speech_threshold: 设低一点，让 Whisper 更不容易将音频判断为无语音\n",
    "        # condition_on_previous_text: 设为 False 可能有助于减少长静音后的重复或幻觉\n",
    "        result = whisper_model.transcribe(\n",
    "            str(long_audio_path),\n",
    "            language=\"ja\",\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            no_speech_threshold=0.5,\n",
    "            condition_on_previous_text=False\n",
    "        )\n",
    "        \n",
    "        logging.info(f\"✅ 长音频转录完成，获取到 {len(result['segments'])} 个字幕片段。\")\n",
    "        return result['segments']\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"❌ 处理长音频文件时发生严重错误。\", exc_info=True)\n",
    "        return None\n",
    "    \n",
    "def generate_srt_from_segments(segments: list, srt_output_path: Path):\n",
    "    \"\"\"根据 Whisper 返回的 segments 列表生成 SRT 文件。\"\"\"\n",
    "    logging.info(f\"正在根据 {len(segments)} 个片段生成 SRT 文件到: {srt_output_path}\")\n",
    "    with open(srt_output_path, 'w', encoding='utf-8') as f:\n",
    "        for i, seg in enumerate(segments, start=1):\n",
    "            start_time = seg['start']\n",
    "            end_time = seg['end']\n",
    "            text = seg['text'].strip()\n",
    "            \n",
    "            # 直接使用我们修改过的 format 函数\n",
    "            start_srt = format_time_for_srt(start_time)\n",
    "            end_srt = format_time_for_srt(end_time)\n",
    "            \n",
    "            f.write(f\"{i}\\n\")\n",
    "            f.write(f\"{start_srt} --> {end_srt}\\n\")\n",
    "            f.write(f\"{text}\\n\\n\")\n",
    "    logging.info(f\"✅ SRT 字幕文件已成功保存。\")\n",
    "\n",
    "# ===================================================================\n",
    "# 测试区 (已适配最终简化流程)\n",
    "# ===================================================================\n",
    "logging.info(\"--- [测试开始] 步骤4：ASR语音识别 (使用 Whisper 原生分割) ---\")\n",
    "\n",
    "try:\n",
    "    # 假设这是您在前几个单元格中定义的变量\n",
    "    video_stem = \"vocals\" \n",
    "    vocals_16k_mono_path = TEMP_DIR / f\"{video_stem}_16k_mono.wav\"\n",
    "    \n",
    "    if vocals_16k_mono_path.exists():\n",
    "        whisper_model = load_original_whisper_model()\n",
    "        \n",
    "        if whisper_model:\n",
    "            # 直接处理这个长音频文件\n",
    "            transcription_segments = transcribe_long_audio_with_whisper(whisper_model, vocals_16k_mono_path)\n",
    "            \n",
    "            if transcription_segments is not None:\n",
    "                japanese_srt_path = OUTPUT_DIR / f\"{video_stem}_ja.srt\"\n",
    "                generate_srt_from_segments(transcription_segments, japanese_srt_path)\n",
    "                \n",
    "                logging.info(\"\\\\n--- [测试成功] 步骤4全部完成 ---\")\n",
    "                logging.info(\"生成的日语字幕文件预览 (前5条):\")\n",
    "                for item in transcription_segments[:5]:\n",
    "                    start = item['start']\n",
    "                    end = item['end']\n",
    "                    text = item['text']\n",
    "                    print(f\"[{start:.2f}s --> {end:.2f}s] {text}\")\n",
    "            else:\n",
    "                logging.warning(\"--- [测试警告] ASR 未能从音频中识别出任何文本或处理失败。---\")\n",
    "            \n",
    "            # 清理模型，释放显存\n",
    "            del whisper_model\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    else:\n",
    "        logging.error(f\"--- [测试失败] 找不到优化后的人声音频文件: {vocals_16k_mono_path}\")\n",
    "\n",
    "except NameError as e:\n",
    "    logging.error(f\"--- [测试失败] 变量未定义: {e}。请确保前面的单元格已成功运行。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fa1b6c-0e67-47a2-9b3a-845ccdcb041b",
   "metadata": {},
   "source": [
    "## 五 翻译"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3f5cf91-c038-421a-bce5-f26b04d54448",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-15 01:56:03 - INFO - --- [测试开始] 步骤5：字幕翻译 ---\n",
      "2025-07-15 01:56:03 - INFO - --- 开始翻译SRT文件: vocals_ja.srt ---\n",
      "2025-07-15 01:56:03 - INFO - 成功从 'vocals_ja.srt' 解析出 376 个字幕条目。\n",
      "2025-07-15 01:56:03 - INFO -   -> 正在翻译批次 [1-10/376]...\n",
      "2025-07-15 01:56:19 - INFO -   -> 正在翻译批次 [11-20/376]...\n",
      "2025-07-15 01:56:36 - INFO -   -> 正在翻译批次 [21-30/376]...\n",
      "2025-07-15 01:56:52 - INFO -   -> 正在翻译批次 [31-40/376]...\n",
      "2025-07-15 01:57:07 - INFO -   -> 正在翻译批次 [41-50/376]...\n",
      "2025-07-15 01:57:22 - INFO -   -> 正在翻译批次 [51-60/376]...\n",
      "2025-07-15 01:57:36 - INFO -   -> 正在翻译批次 [61-70/376]...\n",
      "2025-07-15 01:57:52 - INFO -   -> 正在翻译批次 [71-80/376]...\n",
      "2025-07-15 01:58:07 - INFO -   -> 正在翻译批次 [81-90/376]...\n",
      "2025-07-15 01:58:21 - INFO -   -> 正在翻译批次 [91-100/376]...\n",
      "2025-07-15 01:58:35 - INFO -   -> 正在翻译批次 [101-110/376]...\n",
      "2025-07-15 01:58:51 - INFO -   -> 正在翻译批次 [111-120/376]...\n",
      "2025-07-15 01:59:06 - INFO -   -> 正在翻译批次 [121-130/376]...\n",
      "2025-07-15 01:59:19 - INFO -   -> 正在翻译批次 [131-140/376]...\n",
      "2025-07-15 01:59:34 - INFO -   -> 正在翻译批次 [141-150/376]...\n",
      "2025-07-15 01:59:49 - INFO -   -> 正在翻译批次 [151-160/376]...\n",
      "2025-07-15 02:00:05 - INFO -   -> 正在翻译批次 [161-170/376]...\n",
      "2025-07-15 02:00:19 - INFO -   -> 正在翻译批次 [171-180/376]...\n",
      "2025-07-15 02:00:37 - INFO -   -> 正在翻译批次 [181-190/376]...\n",
      "2025-07-15 02:00:51 - INFO -   -> 正在翻译批次 [191-200/376]...\n",
      "2025-07-15 02:01:05 - INFO -   -> 正在翻译批次 [201-210/376]...\n",
      "2025-07-15 02:01:23 - INFO -   -> 正在翻译批次 [211-220/376]...\n",
      "2025-07-15 02:01:40 - INFO -   -> 正在翻译批次 [221-230/376]...\n",
      "2025-07-15 02:01:55 - INFO -   -> 正在翻译批次 [231-240/376]...\n",
      "2025-07-15 02:02:15 - ERROR - 调用 DeepSeek API 时发生网络错误: Response ended prematurely\n",
      "2025-07-15 02:02:15 - ERROR - 批次 24 翻译失败，将使用占位符。\n",
      "2025-07-15 02:02:15 - INFO -   -> 正在翻译批次 [241-250/376]...\n",
      "2025-07-15 02:02:28 - INFO -   -> 正在翻译批次 [251-260/376]...\n",
      "2025-07-15 02:02:42 - INFO -   -> 正在翻译批次 [261-270/376]...\n",
      "2025-07-15 02:02:56 - INFO -   -> 正在翻译批次 [271-280/376]...\n",
      "2025-07-15 02:03:10 - INFO -   -> 正在翻译批次 [281-290/376]...\n",
      "2025-07-15 02:03:24 - INFO -   -> 正在翻译批次 [291-300/376]...\n",
      "2025-07-15 02:03:38 - INFO -   -> 正在翻译批次 [301-310/376]...\n",
      "2025-07-15 02:03:59 - ERROR - 调用 DeepSeek API 时发生网络错误: Response ended prematurely\n",
      "2025-07-15 02:03:59 - ERROR - 批次 31 翻译失败，将使用占位符。\n",
      "2025-07-15 02:03:59 - INFO -   -> 正在翻译批次 [311-320/376]...\n",
      "2025-07-15 02:04:13 - INFO -   -> 正在翻译批次 [321-330/376]...\n",
      "2025-07-15 02:04:29 - INFO -   -> 正在翻译批次 [331-340/376]...\n",
      "2025-07-15 02:04:43 - INFO -   -> 正在翻译批次 [341-350/376]...\n",
      "2025-07-15 02:05:02 - INFO -   -> 正在翻译批次 [351-360/376]...\n",
      "2025-07-15 02:05:17 - INFO -   -> 正在翻译批次 [361-370/376]...\n",
      "2025-07-15 02:05:33 - INFO -   -> 正在翻译批次 [371-376/376]...\n",
      "2025-07-15 02:05:52 - INFO - ✅ 中文SRT文件已成功保存到: workspace/output/vocals_zh.srt\n",
      "2025-07-15 02:05:52 - INFO - --- [测试完成] 步骤5执行完毕 ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "中文翻译文件预览 (前100行):\n",
      "1\n",
      "00:00:00,868 --> 00:00:01,724\n",
      "感谢您的观看\n",
      "\n",
      "2\n",
      "00:00:47,236 --> 00:00:49,724\n",
      "果然大海最棒了啊\n",
      "\n",
      "3\n",
      "00:00:51,844 --> 00:00:52,636\n",
      "哦！\n",
      "\n",
      "4\n",
      "00:00:55,460 --> 00:00:57,084\n",
      "来自诗织酱的\n",
      "\n",
      "5\n",
      "00:00:57,828 --> 00:00:58,876\n",
      "诗织？\n",
      "\n",
      "6\n",
      "00:01:06,020 --> 00:01:09,372\n",
      "冲啊青春街道Babystar！\n",
      "\n",
      "7\n",
      "00:01:10,148 --> 00:01:19,196\n",
      "感谢您的观看\n",
      "\n",
      "8\n",
      "00:01:34,564 --> 00:01:39,708\n",
      "月光下穷得叮当响的穷鬼\n",
      "\n",
      "9\n",
      "00:01:41,604 --> 00:01:46,300\n",
      "今天的视频就到这里啦。记得下次也要来看哦！\n",
      "\n",
      "10\n",
      "00:01:46,692 --> 00:01:54,528\n",
      "最近身体撞到卡罗比搞得腰酸背痛，但老子还能继续飙车服务啊！\n",
      "\n",
      "11\n",
      "00:01:54,528 --> 00:02:22,768\n",
      "那天的光芒 让我们再次出发吧 青春街道 狂飙突进正当季 击破希望 就算变成订单也轻松搞定 追寻未知世界 潜水吧 所谓青春 就是不断追逐理想 所谓青春 无论几岁都不会褪色\n",
      "\n",
      "12\n",
      "00:02:22,768 --> 00:02:26,320\n",
      "射门就是 坚持之前说过的话\n",
      "\n",
      "13\n",
      "00:02:26,320 --> 00:02:28,380\n",
      "晚安\n",
      "\n",
      "14\n",
      "00:02:31,076 --> 00:02:32,764\n",
      "大庆哥哥大人\n",
      "\n",
      "15\n",
      "00:02:32,804 --> 00:02:36,476\n",
      "新绿青叶时节 您过得如何\n",
      "\n",
      "16\n",
      "00:02:36,484 --> 00:02:39,472\n",
      "这边是喧嚣的教诲和前提歌渐渐着色\n",
      "\n",
      "17\n",
      "00:02:39,472 --> 00:02:41,980\n",
      "群山已完全换上夏装\n",
      "\n",
      "18\n",
      "00:02:42,756 --> 00:02:45,948\n",
      "自从哥哥大人上京已过去三个月\n",
      "\n",
      "19\n",
      "00:02:45,988 --> 00:02:48,400\n",
      "七日姐姐大人和千纱姐姐大人\n",
      "\n",
      "20\n",
      "00:02:48,400 --> 00:02:51,836\n",
      "没有给叔叔添麻烦吧\n",
      "\n",
      "21\n",
      "00:02:52,452 --> 00:02:55,580\n",
      "你有在认真钻研学问吗？\n",
      "\n",
      "22\n",
      "00:02:55,876 --> 00:02:57,788\n",
      "小栞很担心你\n",
      "\n",
      "23\n",
      "00:02:57,892 --> 00:03:01,052\n",
      "父亲大人和母亲大人也很挂念\n",
      "\n",
      "24\n",
      "00:03:01,380 --> 00:03:03,984\n",
      "其实更希望你能回家看看\n",
      "\n",
      "25\n",
      "00:03:03,984 --> 00:03:05,200\n",
      "如果实在不行的话\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# 5. 导入翻译和SRT处理所需的库\n",
    "# ===================================================================\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# ===================================================================\n",
    "# 步骤 5 的功能函数 (已优化为批量翻译)\n",
    "# ===================================================================\n",
    "\n",
    "# PROMPT_TEMPLATE 在此定义，使用上面优化后的版本\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "# 角色扮演\n",
    "你是一位专业的、深谙日本动漫文化的日语字幕翻译家。你尤其擅长翻译风格夸张、充满笑点、颜艺丰富和大学生日常吐槽的喜剧动漫，比如《碧蓝之海》。\n",
    "\n",
    "# 核心任务\n",
    "你的任务是将我提供的日语字幕文本，逐行翻译成自然、地道、且极具表现力的简体中文。\n",
    "\n",
    "# 术语表 (Glossary) - 必须严格遵守\n",
    "以下是本作品《碧蓝之海》(Grand Blue)中的固定翻译，任何情况下都不能更改：\n",
    "- **核心地点和团体:**\n",
    "  - Grand Blue: Grand Blue (潜水商店名)\n",
    "  - PaB (Peek a Boo): PaB (潜水社团)\n",
    "  - 伊豆大学: 伊豆大学\n",
    "- **主要角色:**\n",
    "  - 北原伊織 (きたはら いおり): 北原伊织\n",
    "  - 古手川千紗 (こてがわ ちさ): 古手川千纱 (注意是“纱”不是“奈”)\n",
    "  - 古手川奈々華 (こてがわ ななか): 古手川奈奈华\n",
    "  - 今村耕平 (いまむら こうへい): 今村耕平\n",
    "  - 浜岡梓 (はまおか あずさ): 滨冈梓\n",
    "  - 吉原愛菜 (よしわら あいな): 吉原爱菜\n",
    "  - 寿竜次郎 (ことぶき りゅうじろう): 寿龙次郎\n",
    "  - 時田信治 (ときた しんじ): 时田信治\n",
    "- **关键物品与活动:**\n",
    "  - スクーバダイビング: 水肺潜水\n",
    "  - ウーロン茶: 乌龙茶 (核心梗！详见下方指南)\n",
    "  - 水: 水 (核心梗！详见下方指南)\n",
    "- **特定语气词/口头禅:**\n",
    "  - ウェイ: 根据语境灵活翻译为“哟—!”、“Wassup!”、“燥起来!”等，体现年轻人聚会时的起哄感，避免生硬的“Wao”。\n",
    "  - チェイサー: (在酒桌上) 解围酒，请勿翻译成“追赶者”。\n",
    "\n",
    "# 翻译风格与核心梗指南\n",
    "1.  **忠于原作搞笑精神**: 这是最高原则！翻译必须能传达出原作的喜剧效果和“有病”感。允许使用贴切的网络流行语、俏皮话和吐槽风格的句子，但要避免过度使用而显得尴尬。\n",
    "2.  **“生命之水”梗**:\n",
    "    - 当角色一本正经地谈论“乌龙茶”或“水”，但情景明显是喝酒时（例如，能点燃），必须在译文中体现出这个笑点。\n",
    "    - 优先方案：直接翻译成“乌龙茶”或“水”，但在后面用括号或注释点明，如：**乌龙茶（可燃）**、**水（生命之水）**。\n",
    "    - 备选方案：用引号强调，如：“乌龙茶”、“水”。\n",
    "3.  **口语化与颜艺匹配**: 角色都是大学生，对话风格随意。请使用自然的现代汉语口语。当画面出现夸张的“颜艺”时，译文的语气和用词也要足够夸张，以达到“音画同步”的喜剧效果。\n",
    "4.  **处理ASR错误**: 输入的日文由ASR生成，可能存在错误。例如，遇到长段重复的无意义字符（如 `あああああ`），请理解其为语气词并恰当截断，如翻译为“啊——！”即可，无需逐字翻译。遇到明显的识别错误，请基于上下文进行合理修正。\n",
    "\n",
    "# 输出格式要求\n",
    "请严格按照以下JSON格式输出，不要添加任何额外的解释、注释或markdown标记。\n",
    "- 格式: 一个JSON数组，每个对象包含 \"id\" (从1开始的原始行号) 和 \"translation\" (翻译后的文本)。\n",
    "- 示例输入: `1: こんにちは\\\\n2: これはテストです`\n",
    "- 示例输出: `[{{ \"id\": 1, \"translation\": \"你好\" }}, {{ \"id\": 2, \"translation\": \"这是一个测试\" }}]`\n",
    "\n",
    "# 待翻译内容\n",
    "现在，请翻译以下内容：\n",
    "{BATCH_CONTENT}\n",
    "\"\"\"\n",
    "def parse_srt_file(srt_path: Path):\n",
    "    \"\"\"解析SRT文件，返回包含{'index', 'time', 'text'}的字典列表。\"\"\"\n",
    "    if not srt_path.exists():\n",
    "        logging.error(f\"SRT文件未找到: {srt_path}\")\n",
    "        return []\n",
    "    with open(srt_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    pattern = re.compile(r'(\\d+)\\n([\\d:,]+ --> [\\d:,]+)\\n(.*?)\\n\\n', re.DOTALL)\n",
    "    matches = pattern.findall(content)\n",
    "    segments = [{'index': m[0], 'time': m[1], 'text': m[2].strip()} for m in matches]\n",
    "    logging.info(f\"成功从 '{srt_path.name}' 解析出 {len(segments)} 个字幕条目。\")\n",
    "    return segments\n",
    "\n",
    "def translate_batch_deepseek(batch_content: str, api_key: str):\n",
    "    \"\"\"使用 DeepSeek API 批量翻译文本。\"\"\"\n",
    "    if not api_key or \"sk-\" not in api_key:\n",
    "        logging.error(\"DeepSeek API 密钥无效或未设置，无法翻译。\")\n",
    "        return None\n",
    "\n",
    "    prompt = PROMPT_TEMPLATE.format(BATCH_CONTENT=batch_content)\n",
    "    url = \"https://api.deepseek.com/chat/completions\"\n",
    "    headers = {\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {api_key}\"}\n",
    "    data = {\n",
    "        \"model\": \"deepseek-chat\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"temperature\": 0.1,\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=data, timeout=180)\n",
    "        response.raise_for_status()\n",
    "        result_text = response.json()['choices'][0]['message']['content']\n",
    "        # 清理和解析返回的JSON\n",
    "        json_match = re.search(r'\\[.*\\]', result_text, re.DOTALL)\n",
    "        if json_match:\n",
    "            return json.loads(json_match.group(0))\n",
    "        else:\n",
    "            logging.error(f\"无法从API响应中解析出JSON数组: {result_text}\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"调用 DeepSeek API 时发生网络错误: {e}\")\n",
    "    except (KeyError, IndexError, json.JSONDecodeError) as e:\n",
    "        logging.error(f\"解析 DeepSeek API 响应时出错: {e}\")\n",
    "    return None\n",
    "\n",
    "def translate_srt_file(ja_srt_path: Path, zh_srt_output_path: Path, api_key: str, batch_size=20):\n",
    "    \"\"\"读取日语SRT，分批翻译所有文本，并保存为中文SRT。\"\"\"\n",
    "    logging.info(f\"--- 开始翻译SRT文件: {ja_srt_path.name} ---\")\n",
    "    segments = parse_srt_file(ja_srt_path)\n",
    "    if not segments: return False\n",
    "\n",
    "    all_translated_segments = {}\n",
    "    total_segments = len(segments)\n",
    "\n",
    "    for i in range(0, total_segments, batch_size):\n",
    "        batch = segments[i:i+batch_size]\n",
    "        batch_text_input = \"\\n\".join([f\"{seg['index']}: {seg['text']}\" for seg in batch])\n",
    "        \n",
    "        logging.info(f\"  -> 正在翻译批次 [{i+1}-{min(i+batch_size, total_segments)}/{total_segments}]...\")\n",
    "        \n",
    "        translated_batch = translate_batch_deepseek(batch_text_input, api_key)\n",
    "        \n",
    "        if translated_batch:\n",
    "            for item in translated_batch:\n",
    "                # API返回的id可能是数字或字符串，统一处理为字符串\n",
    "                all_translated_segments[str(item['id'])] = item['translation']\n",
    "        else:\n",
    "            logging.error(f\"批次 {i//batch_size + 1} 翻译失败，将使用占位符。\")\n",
    "            for seg in batch:\n",
    "                all_translated_segments[seg['index']] = f\"[翻译失败: {seg['text']}]\"\n",
    "        \n",
    "        time.sleep(0.1) # 遵守API使用礼仪，避免过于频繁请求\n",
    "\n",
    "    # 重新构建SRT文件\n",
    "    final_srt_content = []\n",
    "    for seg in segments:\n",
    "        translated_text = all_translated_segments.get(seg['index'], f\"[文本丢失: ID {seg['index']}]\")\n",
    "        final_srt_content.append(f\"{seg['index']}\\n{seg['time']}\\n{translated_text}\\n\")\n",
    "\n",
    "    try:\n",
    "        with open(zh_srt_output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"\\n\".join(final_srt_content))\n",
    "        logging.info(f\"✅ 中文SRT文件已成功保存到: {zh_srt_output_path}\")\n",
    "        return True\n",
    "    except IOError as e:\n",
    "        logging.error(f\"❌ 保存中文SRT文件时出错: {e}\")\n",
    "        return False\n",
    "\n",
    "# ===================================================================\n",
    "# 测试区\n",
    "# ===================================================================\n",
    "logging.info(\"--- [测试开始] 步骤5：字幕翻译 ---\")\n",
    "\n",
    "# 假设这些变量已在之前的单元格中定义和创建\n",
    "# OUTPUT_DIR = Path(\"workspace/output\")\n",
    "# japanese_srt_path = OUTPUT_DIR / \"your_video_ja.srt\" \n",
    "# DEEPSEEK_API_KEY = \"sk-...\"\n",
    "\n",
    "if 'japanese_srt_path' in locals() and japanese_srt_path.exists():\n",
    "    video_stem = japanese_srt_path.stem.replace('_ja', '')\n",
    "    chinese_srt_path = OUTPUT_DIR / f\"{video_stem}_zh.srt\"\n",
    "    \n",
    "    if not DEEPSEEK_API_KEY or \"sk-xxxxxxxx\" in DEEPSEEK_API_KEY:\n",
    "        logging.error(\"--- [测试失败] ---\")\n",
    "        logging.error(\"请在第一个单元格的'全局变量与路径配置'部分填入您的有效 DeepSeek API 密钥。\")\n",
    "    else:\n",
    "        translate_srt_file(japanese_srt_path, chinese_srt_path, DEEPSEEK_API_KEY, batch_size=10)\n",
    "        \n",
    "        logging.info(\"--- [测试完成] 步骤5执行完毕 ---\")\n",
    "        if chinese_srt_path.exists():\n",
    "             with open(chinese_srt_path, 'r', encoding='utf-8') as f:\n",
    "                print(\"\\n中文翻译文件预览 (前100行):\")\n",
    "                # 使用 for 循环安全地读取前10行\n",
    "                preview_lines = []\n",
    "                for _ in range(100):\n",
    "                    line = f.readline()\n",
    "                    if not line:\n",
    "                        break\n",
    "                    preview_lines.append(line)\n",
    "                print(\"\".join(preview_lines))\n",
    "else:\n",
    "    logging.error(\"--- [测试失败] 找不到上一步生成的 'japanese_srt_path' 文件。\")\n",
    "    logging.error(\"请确保步骤4.1的单元格已成功运行。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f309b7-a948-4eb5-8172-7ceacb6cab22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ani_sub",
   "language": "python",
   "name": "ani_sub"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
